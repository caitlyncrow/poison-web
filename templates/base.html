<!DOCTYPE html>
<html lang="en">
<head>

    <!--The below code creates the navigation bar-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Navigation Bar</title>
    <link type="text/css" rel="stylesheet" href="{{ url_for('static', filename='base.css') }}" />
</head>
<body>

    <!--The below code specifies the tabs that the navigation bar will comprised of-->
    <ul class="tabs">
        <li data-tab-target="#home" class="active tab">Home</li>
        <li data-tab-target="#about" class="tab">About</li>
        <li data-tab-target="#types" class="tab">Types</li>
        <li data-tab-target="#identification" class="tab">Identification</li>
        <li data-tab-target="#weaknesses" class="tab">Weaknesses</li>
        <li data-tab-target="#solutions" class="tab">Solutions</li>
        <li data-tab-target="#examples" class="tab">Examples</li>
        <li data-tab-target="#citations" class="tab">Citations</li>
    </ul>

    
    <div class="tab-content">

        <!--Below code displays the content for each tab that I can click on in the navigation bar-->
        <!--By default, the Home tab is active, meaning that it is selected initially-->
        <div id="home" class="section1 active" data-tab-content>
            <h1>Learn more about data poisoning and the ways it can be addressed!</h1>
            <h2>Explore the tabs to learn more!</h2>
        </div>
        
        <div id="about" class="section1" data-tab-content>
            <h1>
                <b>About Data Poisoning</b>
            </h1>
            <br> 
            <h2>
                <b>What is Data Poisoning?</b>
            </h2>
            <p class = "big_para">Data Poisoning is an adversarial attack on machine learning models where the attacker targets the model's training data
            to negatively affect the model's performance. This change in performance can be as simple as a reduction in the model's
            accuracy to something as malicious as generating hateful and dangerous output in favor of the attacker.
            </p>
            <br>
            <h2>
                <b>The Dark Side of Data Poisoning</b>
            </h2>
            <p class = "big_para">Data Poisoning is considered to be the most critical vulnerability for machine learning models, such as Large
            Language Models (LLMs), and for good reason. This particularly holds true when looking at the strong influence data
            poisoning has on information consumption. As discussed in the section <b>What is Data Poisoning?</b>, a
            Data Poisoning attack leads to an LLM or any kind of machine learning model to become corrupted and therefore
            make inaccurate predictions that perpetuate information inconsistent with the training data. This results in
            the presence of misinformation or at worse disinformation, where the attacker purposefully wants the model
            to communicate certain information that is inconsistent with the training data.
            </p>
            <br>
            <h2>
                <b>Is Data Poisoning Really All That Bad?</b>
            </h2>
            <p class = "big_para">Ironically, Data Poisoning can actually be used in a manner intended to benefit
            machine learning models. Take for example the Data Poisoning Attack called <i>Nightshade</i>. It was an attack developed
            by a group of researchers that is now used by various content creators, such as artists, to protect their work from being used
            in other data sets without permission.
            </p>
        </div>
        
        
        <div id="types" class="section1" data-tab-content>
            <h1>
                <b>Types of Data Poisoning</b>
            </h1>
                <p>There are five main types of data poisoning attacks, 
                which are Availability Attacks, Backdoor Attacks, Targeted Attacks, Subpopulation Attacks, and 
                "Dirty Neighbor" Attacks.</p>
                <br> 
            <h2>
                <b>Availability Attack</b>
            </h2>
                <p class = "big_para">The entire model gets corrupted, which means there is an 
                increase in the number of false negatives and false positives for the predictions.</p>
            <br>
            <h2>
                <b>Backdoor Attack</b>
            </h2>
                <p class = "big_para">The model gets introduced to several examples that cause it to 
                misclassify the predictions from the training data, which results in the model being corrupted. 
                In order for the attacker to decide on which examples to use, they hack into the model’s information, 
                such as its training data. Unlike an Availability Attack, a Backdoor Attack is purposeful in 
                deciding which data samples to poison, whereas an Availability Attack may involve poisoning randomized data samples 
                without necessarily having knowledge of the actual data.</p>
            <br>
            <h2>
                <b>Targeted Attack</b>
            </h2>
                <p class = "big_para">The model accurately classifies most of the training data, but there are a few examples 
                that get misclassified. Because of this, a Targeted Attack can be deceiving and especially hard to detect. 
                Just like a Backdoor Attack, a Targeted Attack is purposeful in deciding which data samples to poison. 
                However, a Targeted Attack goes a step further by poisoning data in such a way so that the model conveys the exact 
                information the attacker wants.</p>
                <p class = "big_para">Microsoft Tay is an excellent example of a Targeted Attack at play – check out the <a data-tab-target="#weaknesses" class = "link">Weaknesses</a> tab to learn more</p>
                <!--The above content corresponding to Targeted Attack includes a link to the Weaknesses tab-->
            <br>
            <h2>
                <b>Subpopulation Attack</b>
            </h2>
                <p class = "big_para">The model undergoes a Targeted Attack, but in addition to affecting those few examples, 
                this attack also affects examples that are conceptually similar. For example, when talking about an image
                generation model, the model may produce images of cats as opposed to dogs. However, a Subpopulation
                Attack may also affect concepts related to "dog", such as "wolf", "puppy", etc.</p>
            <br>
            <h2>
                <b>The “Dirty Neighbor” Attack</b>
            </h2>
                <p class = "big_para">This attack takes particular advantage of the relationship between semantic frequency and concept sparsity 
                (refer to the tab <a data-tab-target="#weaknesses" class = "link">Weaknesses</a> for more information). This attack works by selecting a concept that is the 
                complete opposite or unrelated to the original concept. From there, the attack poisons the semantic “neighbors” 
                of this opposite or unrelated concept, so that this concept now becomes
                semantically similar to the original concept. In essence, antonyms now become synonyms, 
                and the predictions become nonsensical. This method is highly effective and 
                requires only a small number of poisoning samples to be detrimental to the model.</p>
                <!--The above content corresponding to the "Dirty Neighbor" attack includes a link to the Weaknesses tab-->
            <br>

        </div>

        
        <div id="identification" class="section1" data-tab-content>
            <h1>
                <b>How To Identify Data Poisoning in Large Language Models</b>
            </h1>
            <br> 
            <h2>
                <b>What does poisoned data look like?</b>
            </h2>
                <p class = "big_para"> Data poisoning adds another level of difficulty in filtering out bad data. After all, 
                it wouldn’t be very effective if the bad data was easy to identify. 
                Even evaluating the performance of a model is not very effective in determining if the model has been poisoned, 
                much less identifying the offending samples. This is in part because there is no base performance standard 
                for every model domain and data set type. This is especially true for LLMs. 
                It is no simple task to quantify the performance of language output. Ultimately, the performance quantifiers are only 
                useful in comparison with the quantifiers of the same model. </p>
            <br>
            <h2>
                <b>Major Challenges</b>
            </h2>
                <p class = "para">1. Data poisoning attacks can be pretty simple to implement, because they don’t always need access to the image generation pipeline, 
                large data samples, or knowledge of how the model is trained in order for them to be successful.</p>
                <p class = "para">2. Oftentimes, it is challenging to determine if an LLM model has a data poisoning attack in the first place, since it’s 
                hard to evaluate its performance. As previously mentioned in the section <b>What does poisoned data look like?</b>, 
                there is not one way to measure a model’s performance. Rather, the model’s performance 
                can be evaluated in different ways. For instance, one way a model’s performance can be measured is using the PCA method.</p>
                <h3 style="margin-left: 40px">
                    <b>What is the PCA method?</b>
                </h3>
                <p class = "para">The PCA method aims to retain a model’s accuracy using the fewest variables possible. The few variables that are left are the ones with the lowest variance, 
                which indicates that they have the most significant role compared to the rest of
                the variables in affecting a model’s predictions and ultimately its performance. </p>
                <br>
                <p class = "big_para">Additionally, a model’s performance can be hard to evaluate because of the sheer amount of data the model needs to use. 
                Since a machine learning model relies on so much data for making predictions, it becomes challenging to maintain quality control 
                of the model, thereby making it challenging to evaluate its performance.
                Furthermore, a lot of that data gets scraped from the Internet, making it hard to maintain and ensure the data is used 
                for the right purposes.</p>
        </div>
        
        
        <div id="weaknesses" class="section1" data-tab-content>
            <h1>
                <b>Understanding What Can Make a Large Language Model (LLM) Susceptible to Data Poisoning</b>
            </h1>
            <br>
            
            <h2>Failed vs. Successful Models</h2>
            <h3 style="margin-left: 40px">Failed Model</h3>

            <p class = "big_para">A major failure was an LLM model produced by Microsoft called Tay. Its failure is said to be
            mainly influenced from the way the model was trained. In essence, Tay was trained by generating responses to various
            questions it was asked. For every response it generated, Tay mapped the generated responses to their corresponding
            questions as well as questions that were semantically similar.</p>

            <h4 style="margin-left: 80px">
                <b>What is Meant by Semantically Similar?</b>
            </h4>
            <p class = "para">Let's consider the question "What is your name?". The question "What is your name?"
            can be asked in a variety of ways, such as "Who are you?", "What do you go by?", etc. The responses to these
            questions are generally going to be the same. For instance, the response "My name is Sam" or "I'm Sam" can work
            for each of these questions. Since all the questions are similar to one another and produce pretty much the same
            responses, these questions are therefore said to be semantically similar.</p>

            <p class = "big_para">Even though Tay was trained in an effective manner, a major issue was that
            two key parameters weren't specified: Type and Frequency. Specifically, the model mapped questions to responses
            too broadly. Due to its lack of constraints that would be indicated by the parameters, the model became subject
            to Targeted Attacks (refer to the tab <a data-tab-target="#types" class = "link">Types</a> for more detail). Ultimately, this made the model vulnerable
            to inappropriate and offensive responses directed by users with malicious intent.</p>
            <!--The above content corresponding to the What is Meant by Semantically Similar section includes a link to the Types tab-->
            
            <h3 style="margin-left: 40px">
                <b>Model Training Parameters</b>
            </h3>
           
            <h4 style="margin-left: 80px">
                <b>Type</b>
            </h4>
            
            <p class = "para">This parameter specifies the type of input the model
            receives (i.e. the type of questions Tay gets asked in this case).</p>
            <h4 style="margin-left: 80px">
                <b>Frequency</b>
            </h4>
            <p class = "para">This parameter specifies how often the model receives various types
            of input (i.e. how often Tay gets asked certain types of questions).</p>
            <p class = "big_para">These parameters can be the key to mitigating Data Poisoning attacks,
            as placing constraints on the type and frequency of the input the model receives can help filter out the attacks.
            For instance, if the model was designed to only accept input that doesn't include remarks that are deemed offensive, 
            then any input that included offensive remarks would be considered as outliers. This is discussed in greater
            detail in the <a data-tab-target="#solutions" class = "link">Solutions</a> tab, but determining outliers can help filter out poisoned data samples, since
            the poisoned data samples will "stand out" from the "normal" data samples.</p>
            <!--The above content corresponding to the Frequency section includes a link to the Solutions tab-->
        <br>
        <h3 style="margin-left: 40px">
            <b>Successful Model</b>
        </h3>
        <p class = "big_para">Unlike Tay, the model known as XiaoIce was much more successful in its design.
        Its success mainly has to do with the model taking into account the context and the use of language when 
        producing responses to the questions it gets asked. In essence, the design of XiaoIce takes in components
        of EQ in addition to IQ. Most machine learning models only focus on IQ, which models the concept of memory and
        the ability to "learn" by making sense out of prior knowledge. However, it's the EQ that takes into account
        the context and use of language.</p>
        <h3 style="margin-left: 40px">
            <b>Specific Design Components of XiaoIce</b>
        </h3>
        <h4 style="margin-left: 80px">
            <b>Emotion</b>
        </h4>
            <p class = "para">On top of the data associated with the questions it receives from
            the user, XiaoIce also gets pre-trained on data pertaining to common human emotions. This is to ensure that
            XiaoIce is able to recognize those emotions and similar ones to them when being trained.</p>
        <h4 style="margin-left: 80px">
            <b>Context</b>
        </h4>
            <p class = "para">In order to train on common human emotions as well as questions it may be
            asked, the model uses various querying techniques such as Markov Decision Processes to filter out all
            the possible emotions a given person is experiencing based on their use of language.</p>
        <br>
        <h3 style="margin-left: 40px">
            <b>What are Markov Decision Processes?</b>
        </h3>
            <p class = "big_para">Markov Decision Processes are a means for a machine learning model to
            determine the possible next states that can occur based on the current state and previous states
            that occurred. In the case of XiaoIce, the states represent the various words provided as input
            for which emotions are predicted. Therefore, the current state would be the current word
            for which XiaoIce is making a prediction and the previous states would be the words XiaoIce
            already took as input to make predictions. Meanwhile, the future states would be the next remaining words
            in the input that can be used to predict certain emotions. The fewer remaining words there are left in the input, 
            the more constrained (i.e. queried) the possible emotions are. Eventually, this results in one or a few
            possible emotions being left, which is used to predict the entire output. By using query techniques such as 
            Markov Decision Processes, XiaoIce is able to determine the context as well as the emotional level of that given 
            conversation and thereby offer an appropriate response.</p>
        <br>
            <p class = "big_para">By taking emotion and ultimately context into account, XiaoIce is able
            to significantly reduce the occurrence of inappropriate and offensive responses, as they
            are responses that are typically not viewed positively by the user.</p>
        <br>
        <h2>
            <b>Weaknesses Caused By Data</b>
        </h2>
        <h3 style="margin-left: 40px">
            <b>Bad Data</b>
        </h3>
        <p class = "big_para">LLMs, like most modern machine learning models, rely heavily on lots of data.
        However, these models may not have inherent defense strategies to identify bad data. Data is considered to
        be bad when it is incorrectly labeled, missing key information, contains irrelevant information, or even
        considered harmful to the accuracy and performance of the model. Unfortunately, every machine learning model
        will be trained on some amount of bad data, regardless of whether that data is poisoned. Yet, this is
        exactly why it's important for a machine learning model to be trained on such vast amounts of data: 
        having the intention of good, viable data to overpower the bad data.</p>
        <br>
        <h3 style="margin-left: 40px">
            <b>Weak Data</b>
        </h3>
        <p class = "big_para">It's not only bad data that may potentially negatively affect
        a model's performance. Additionally, weak data is data that provides little use to the model yet
        can provide an opening for a Data Poisoning attack to exploit the model.
        A Data Poisoning attack may exploit important features of the model, such as Semantic Frequency
        and Concept Sparsity.</p>
        <h4 style="margin-left: 80px">
            <b>Concept Sparsity</b>
        </h4>
        <p class = "para">Concept Sparsity is the idea that particular concepts (e.g. words
        for which an LLM model is trained on) may have smaller pools, or clusters, to draw from than other concepts
        in the training data. Since these concepts are in small clusters, they are therefore
        appealing for data poisoning attacks. This is because ultimately few data samples are needed
        in order for a Data Poisoning attack to disrupt the model. Once these few data samples become
        infected, the attack can bleed over to concepts that are semantically similar to those from the small pools.
        For instance, if the concept "dog" and "puppy" are from one small cluster and they get infected,
        the attack is also going to negatively affect related concepts, such as "wolf", "bear", etc; this
        may ultimately corrupt the entire model.</p>
        <h4 style="margin-left: 80px">
            <b>Semantic Frequency</b>
        </h4>
        <p class = "para">Semantic Frequency is the extent towards which certain concepts
        are related to each other. This relationship between concepts can be 
        taken advantage of to further spread the influence of poisoned data. For instance, let's
        consider a data poisoning attack affects the concept "castle". Any time the model encounters the
        word "castle" in its training data, it falsely interprets the meaning of "castle" to be closer to
        "toaster". As a result, the model may cascade into wrong predictions towards concepts 
        related to "castle", such as "house". Now, the model assumes houses are made of metal and its denizens
        are bread, which is absurd!</p>
        <br>
        <h2>
            <b>Weaknesses Caused By a Model's Training Structure</b>
        </h2>
        <p class = "big_para">As mentioned in the section titled <b>Weaknesses Caused By Data</b>, most machine
        learning models are trained using vast amounts of data.
        There are two main types of machine learning models, which are Supervised and Unsupervised
        machine learning models. The main difference between these two types of models is the use
        of labeled input in Supervised models, whereas Unsupervised models don't use labels to mark
        their input. However, regardless of the type of model, they can both be vulnerable
        to Data Poisoning attacks, because they both have a particular goal in mind (i.e. ability to
        take in input to make predictions).</p>



    </div>
    
    
    <div id="solutions" class="section1" data-tab-content>
            <h1>Possible Solutions to Address Data Poisoning</h1>
            <br> <br>
            <h2>Improving with Sparsity</h2>
            
            <p class = "big_para">
                Concept sparsity can be leveraged to create a better model. 
                <a data-tab-target="#types" class = "link">Backdoor attacks</a> can be used to identify the frequency of concepts within a dataset 
                to then target those with lower frequency. 
                This same idea can be used to identify weaknesses in data and provide opportunities to 
                improve the model's organization of such data. In this way, the model can improve its accuracy without requiring a 
                substantial increase in computing
                power. Furthermore, unstructured sparsity is a technique used within the structure of the model by 
                keeping certain weights at zero, which essentially prunes the connections within the model. 
                This is particularly useful in increasing the model’s accuracy.</p>
                <!--The above content corresponding to the Improving with Sparsity section includes a link to the Types tab-->
                
            <p class = "big_para">More information about using sparsity to identify bad data can be found in the <a data-tab-target="#weaknesses" class = "link">Weaknesses</a> 
            tab under the <b>Weaknesses Caused by Data</b> heading.</p>
            <!--The above content corresponding to the Improving with Sparsity section includes a link to the Weaknesses tab-->
            <br>
            <br>
            <h2>
                <b>Alternative Training Techniques</b>
            </h2>
            
            <h3 style="margin-left: 40px"><b>Distillation</b></h3>
                <p class = "para">Distillation involves training a model not on data, 
                but on another model. This “teacher” model tends to be larger and more generic than the “student” model,
                which tends to resemble a fine-tuned model in its narrow scope. 
                The teacher model is trained using normal techniques and a generic dataset, and the student model 
                is trained on the outputs of the teacher model with an additional context-appropriate dataset. 
                This student model uses less computational power while 
                maintaining its accuracy. It’s near impervious to data poisoning because its goal is not to make accurate predictions 
                but to mimic the functionality of the teacher model. While this is a powerful technique, 
                it is important to note that the resulting model is smaller in scope and functionality than the 
                large generic teacher model. It may result in accurate predictions using a fraction of memory, 
                but it is not powerful enough to build a model to rival something like ChatGPT.</p>
            <br>
            <h3 style="margin-left: 40px"><b>In-Context Learning (ILC)</b></h3>
                <p class = "para">In-context learning, or ILC, utilizes information given in the prompt to give additional context to the model. This additional context, assuming it is appropriately given, is one way users can get more accurate results from LLMs;
                 even if the model has been poisoned. It works by giving the model additional context or information on the topic the prompt is addressing. This can look like giving examples of the information you want, for example, “Grass is green, bark is brown, the sky is…”. 
                 It can also appear as directing the model to “take it step by step”. For example, LLMs are designed for language, not arithmetic. Often these models, when prompted directly to solve simple arithmetic, fail miserably. However, by asking the model to write out its
                  process in detail (or “step-by-step”!) this kind of “zero-shot” reasoning has been proven to help the same model correctly respond to the same question it would have otherwise gotten wrong.</p>
            <br>
            <h3 style="margin-left: 40px"><b>Adversarial Training</b></h3>
                <p class = "para">Adversarial training is a way in which data poisoning can be used to build a more robust and resilient model. This type of training involves purposefully adding bad data or changing the reward feedback in an effort to fool the model into 
                making incorrect predictions. This may sound counterproductive at first, but this technique allows model architects to identify weak points in the model and update parameters. Likewise, training the model to identify and handle bad data can make it more difficult 
                for the model to be affected by adversarial attacks or data poisoning later on. However, adversarial training is expensive because it requires a lot of data and computing power. In some cases, this training technique can reduce accuracy due to the tendency of these 
                models to become overfitted. However, there is a similar technique that doesn’t have these weak points.</p>
            <br>
            <h3 style="margin-left: 40px"><b>Adversarial Augmentation</b></h3>
                <p class = "para"> Adversarial augmentation trains the model on data in an effort to teach the model how to identify distorted or harmful data. Augmentation can be used in tandem with adversarial training to reduce overfitting, but only a few such
                 methods have been found to produce satisfactory results. Models often find “shortcuts” or irrelevant correlations in the data that can reduce accuracy and be taken advantage of by poisoning attacks. Adversarial augmentation reduces the influence of such noise, and 
                 therefore weakens the efficacy of such attacks. </p>
            <br> <br>
            <h2>Outlier Detection For Removing Poisoned Data</h2>
                <p class = "big_para">Outlier detection techniques can help in 
                removing poisoned data since they are effective at filtering various data samples, 
                such as poisoned data samples. This is because outliers are data values that are outside of “normal behaviors”, 
                making them clearly identifiable due to them standing out from the rest of the data.</p>
            <br>
            <h3 style="margin-left: 40px">Z-Score</h3>
                <p class = "para">The z-score is a common technique used for detecting outliers. Specifically, the z-score measures the number of standard deviations a particular data point is away from the median pertaining to all the data. If there are 3 or more standard deviations for a given data point,
                     then that data point is an outlier. The quantity of the outliers, as well as the number of standard deviations away, can have a significant impact on the performance of an LLM. However, the z-score method is simply one way to detect outliers.</p>
            <br>
            <h3 style="margin-left: 40px">Local Outlier Factor</h3>
                <p class = "para">Another common method used for detecting outliers in more complex and larger data is known as the Local Outlier Factor. The Local Outlier Factor is used to measure the density deviation of a given data sample compared to its neighboring data samples. Density refers to the number of points a
                     data sample contains. In essence, the data sample can be thought of as a cluster; the more compact the cluster, the higher the density. In order to determine the density deviation, it’s typical to use the K-Nearest Neighbors machine learning algorithm to determine the nearest sample neighbors of a given data sample.
                      If the given sample has a low density but the neighboring samples have high densities, then that means the given sample has a high density deviation; a high density deviation indicates the presence of outliers in the data sample.</p>
            <br>
            <h2><b>Defenses Against Poisoned Data</b></h2>
                <p class = "big_para">The defenses against data poisoning tend to be reactionary. There are no well-developed methods for consistently identifying poisoned data within the training data or the model. However, there are plenty of identification and pruning techniques to handle the poisoned data that has already been introduced. 
                    We explore the techniques in depth in the <a data-tab-target="#identification" class = "link">Identification</a> tab.</p>
                    <!--The above content corresponding to the Defenses Against Poisoned Data section includes a link to the Identification tab-->
 
            <br>
            <br>

        </div>

        
        <div id="examples" class="section1" data-tab-content>
            <h1>Below are examples of how a data poisoning attack may look like in code</h1>
            <h2>All of our code and data can be found <a href="https://github.com/caitlyncrow/poison-web">here</a></h2>
            <br>
            <h2>Click any of the links below corresponding to look at specific examples of various data poisoning attacks
            being implemented
            </h2>
            <br> <br>
            <h3 style="margin-left: 40px"><a href="https://colab.research.google.com/drive/11_mXqpvO55YOBsv9TWu4Uzf17cx03nnw?usp=sharing">Backdoor Attack</a></h3>
            <h4 style="margin-left: 40px"><u>Note:</u> Refer to the section titled <i>Implementing a Backdoor Attack</i> in the code.</h4>
            <p class = "para">When looking at the data used in this example, any kind of Glaucoma is a prevalent type of disorder. 
            A Glaucoma disorder refers to a disorder that negatively affects vision and the eyes as a whole.
            A <a data-tab-target="#types" class = "link">Backdoor Attack</a> can introduce a few data samples where Glaucoma gets 
            tied to a disorder associated with hearing and the ears as a whole.</p>
            <img src="/static/images/BackdoorAttack.jpg" class="center">

            <h3 style="margin-left: 40px"><a href="https://colab.research.google.com/drive/11_mXqpvO55YOBsv9TWu4Uzf17cx03nnw?usp=sharing">Subpopulation Attack</a></h3>
            <h4 style="margin-left: 40px"><u>Note:</u> Refer to the section titled <i>Implementing a Subpopulation Attack</i> in the code.</h4>
            <p class="para">In the same code as linked above, the model can be further poisoned with a <a data-tab-target="#types" class = "link">Subpopulation Attack</a>. 
            This goes further than a Backdoor Attack by introducing a few wrong data samples into similar concepts. 
            In this case, the concept of "Glaucoma" was poisoned using a Backdoor Attack; a Subpopulation Attack will seek to poison related concepts, 
            which would be those pertaining to other types of eye diseases/disorders.</p>
            <img src="/static/images/SubpopulationAttack.jpg" class="center">

            <h3 style="margin-left: 40px"><a href="https://colab.research.google.com/drive/11_mXqpvO55YOBsv9TWu4Uzf17cx03nnw?usp=sharing">The "Dirty Neighbor" Attack</a></h3>
            <h4 style="margin-left: 40px"><u>Note:</u> Refer to the section titled <i>Implementing the "Dirty Neighbor" Attack</i> in the code.</h4>
            <p class="para">When referring to the very same code, now let's go a step even further by poisoning data samples pertaining 
            to actual ear disorders by making these samples more similar to eye disorders. 
            By doing this, the model should then be completely fooled by determining what is actually an eye vs. ear disorder.</p>
            <img src="/static/images/TheDirtyNeighborAttack.jpg" class="center">
        
        </div>
        <!--The above section links the code we used to implement data poisoning attacks, as well as uploads images of some of
        the code-->
        
        
        <div id="citations" class="section1" data-tab-content>
            <h1><b>Works Cited</b></h1>
            <br> <br>
            <ul>
                <li>A Comprehensive Introduction to Anomaly Detection. datacamp. (2023, November). <br> 
                    <p style="margin-left: 40px">https://www.datacamp.com/tutorial/introduction-to-anomaly-detection</p>
                </li>
                <li>Dhar, P. (2023, March 24). Protecting AI Models from “Data Poisoning.” IEEE Spectrum. <br> 
                    <p style="margin-left: 40px">https://spectrum.ieee.org/ai-cybersecurity-data-poisoning</p>
                </li>
                <li>Espinosa, N. (2023, March 10). The Unforeseen Consequences of ChatGPT. Forbes. <br> 
                    <p style="margin-left: 40px">https://www.forbes.com/sites/forbestechcouncil/2023/03/10/the-unforeseen-consequences-of-chatgpt/?sh=6835c791eeab</p> 
                </li>
                <li>Hinton, G., et al. (2015, March 9). Distilling the Knowledge in a Neural Network. ArXiv. <br> 
                    <p style="margin-left: 40px">https://arxiv.org/pdf/1503.02531.pdf</p>
                </li>
                <li>Jaadi, Z. (2024, February 23). A Step-by-Step Explanation of Principal Component Analysis (PCA). builtin. <br> 
                    <p style="margin-left: 40px">https://builtin.com/data-science/step-step-explanation-principal-component-analysis</p> 
                </li>
                <li>Jagtap, R. (2022, November 21). Understanding the Markov Decision Process (MDP). builtin. <br>
                    <p style="margin-left: 40px">https://builtin.com/machine-learning/markov-decision-process</p>
                </li>
                <li>Jayaswal, V. (2020, August 30). Local Outlier Factor (LOF). Towards Data Science. <br>
                    <p style="margin-left: 40px">https://towardsdatascience.com/local-outlier-factor-lof-algorithm-for-outlier-identification-8efb887d9843</p> 
                </li>
                <li>Lenaerts-Bergmans, B. (2024, March 20). Data Poisoning: The Exploitation of Generative AI. <br> 
                    <p style="margin-left: 40px">https://www.crowdstrike.com/cybersecurity-101/cyberattacks/data-poisoning/</p>
                </li>
                <li>Li, L., Spratling, M. (2023, Jan 24). Data Augmentation Alone Can Improve Adversarial Training. ArXiv. <br>
                    <p style="margin-left: 40px">https://arxiv.org/pdf/2301.09879.pdf</p>
                </li>
                <li>Lie, Sean. (2023, March 22). Can Sparsity Make AI Models More Accurate? Cerebras. <br> 
                    <p style="margin-left: 40px">https://www.cerebras.net/blog/can-sparsity-make-ai-models-more-accurate</p> 
                </li>
                <li>Paudice, A, et al. (2018, Feb 8). Detection of Adversarial Training Examples in Poisoning Attacks through Anomaly Detection. ArXiv. <br> 
                    <p style="margin-left: 40px">https://arxiv.org/pdf/1802.03041.pdf</p>
                </li>
                <li>Shan, S., Ding, W., Passananti, J., Wu, S., Zheng, H., & Zhao, B. Y. (2024, February 16). Prompt-Specific Poisoning Attacks on Text-to-Image Generative Models. ArXiv. <br> 
                    <p style="margin-left: 40px">https://arxiv.org/pdf/2310.13828.pdf</p> 
                </li>
                <li>Simons, A. (2023, October 23). Data Poisoning: The Newest Threat to Generative AI. Forcepoint. <br> 
                    <p style="margin-left: 40px">https://www.forcepoint.com/blog/x-labs/data-poisoning-gen-ai#:~:text=Types%20of%20data%20poisoning%20attacks,negatives%2C%20and%20misclassified%20test%20samples.</p>
                </li>
                <li>Sinders, C. (2016, March 24). Microsoft’s Tay is an Example of Bad Design. Medium. <br> 
                    <p style="margin-left: 40px">https://medium.com/@carolinesinders/microsoft-s-tay-is-an-example-of-bad-design-d4e65bb2569f#.s8fxcpeqk</p>
                </li>
                <li>Singh, R., Ktisma, S. (2023, Dec 16). VonGoom: A Novel Approach for Data Poisoning in Large Language Models. DelComplex. <br> 
                    <p style="margin-left: 40px">https://xiosky.com/delmedia/media/ documents/vonGoom2023.pdf</p>
                </li>
                <li>Yerlikaya, F. A., & Bahtiyar, S. (2021, October 24). Data poisoning attacks against machine learning algorithms. ScienceDirect. <br> 
                    <p style="margin-left: 40px">https://www.sciencedirect.com/science/article/pii/S0957417422012933</p> 
                </li>
                <li>Zhou, L., Gao, J., Li, D., & Shum, H.-Y. (2020, March 1). The Design and Implementation of Xiaoice, an Empathetic Social Chatbot. MIT Press Direct. <br> 
                    <p style="margin-left: 40px">https://direct.mit.edu/coli/article/46/1/53/93380/The-Design-and-Implementation-of-XiaoIce-an</p> 
                </li>
            </ul>
        </div>
    </div>

    <!--references the javascript file that navigates to the appropriate pages once
    a given tab is clicked-->
    <script src="{{ url_for('static', filename='base.js') }}" defer></script>
    
    
</body>
</html>